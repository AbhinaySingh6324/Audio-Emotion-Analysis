{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-15T05:14:26.153118Z","iopub.execute_input":"2022-08-15T05:14:26.154401Z","iopub.status.idle":"2022-08-15T05:14:26.161542Z","shell.execute_reply.started":"2022-08-15T05:14:26.154350Z","shell.execute_reply":"2022-08-15T05:14:26.160590Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"TESS = \"/kaggle/input/speech/TESS Toronto emotional speech set data/\"\nRAV = \"/kaggle/input/speech1/audio_speech_actors_01-24/\"\n#SAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n#CREMA = \"/kaggle/input/cremad/AudioWAV/\"\n\n# Run one example \ndir_list = os.listdir(RAV)\ndir_list[0:5]\n#'a' = 'anger'\n#'d' = 'disgust'\n#'f' = 'fear'\n#'h' = 'happiness'\n#'n' = 'neutral'\n#'sa' = 'sadness'\n#'su' = 'surprise'","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:26.188858Z","iopub.execute_input":"2022-08-15T05:14:26.189528Z","iopub.status.idle":"2022-08-15T05:14:26.203359Z","shell.execute_reply.started":"2022-08-15T05:14:26.189485Z","shell.execute_reply":"2022-08-15T05:14:26.201716Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n#         print(f)\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:26.241667Z","iopub.execute_input":"2022-08-15T05:14:26.242664Z","iopub.status.idle":"2022-08-15T05:14:26.292119Z","shell.execute_reply.started":"2022-08-15T05:14:26.242624Z","shell.execute_reply":"2022-08-15T05:14:26.290973Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"fname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveshow(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:26.294700Z","iopub.execute_input":"2022-08-15T05:14:26.295188Z","iopub.status.idle":"2022-08-15T05:14:27.047367Z","shell.execute_reply.started":"2022-08-15T05:14:26.295140Z","shell.execute_reply":"2022-08-15T05:14:27.046037Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"# Pick a happy track\nfname = RAV + 'Actor_14/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveshow(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:27.049577Z","iopub.execute_input":"2022-08-15T05:14:27.050076Z","iopub.status.idle":"2022-08-15T05:14:27.799559Z","shell.execute_reply.started":"2022-08-15T05:14:27.050038Z","shell.execute_reply":"2022-08-15T05:14:27.798151Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:27.801933Z","iopub.execute_input":"2022-08-15T05:14:27.802698Z","iopub.status.idle":"2022-08-15T05:14:27.812803Z","shell.execute_reply.started":"2022-08-15T05:14:27.802646Z","shell.execute_reply":"2022-08-15T05:14:27.811412Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:27.815861Z","iopub.execute_input":"2022-08-15T05:14:27.816221Z","iopub.status.idle":"2022-08-15T05:14:27.849160Z","shell.execute_reply.started":"2022-08-15T05:14:27.816188Z","shell.execute_reply":"2022-08-15T05:14:27.847920Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"fname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveshow(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:27.850800Z","iopub.execute_input":"2022-08-15T05:14:27.851131Z","iopub.status.idle":"2022-08-15T05:14:28.572945Z","shell.execute_reply.started":"2022-08-15T05:14:27.851101Z","shell.execute_reply":"2022-08-15T05:14:28.571461Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([RAV_df, TESS_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:28.574972Z","iopub.execute_input":"2022-08-15T05:14:28.575341Z","iopub.status.idle":"2022-08-15T05:14:28.603474Z","shell.execute_reply.started":"2022-08-15T05:14:28.575306Z","shell.execute_reply":"2022-08-15T05:14:28.602549Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"/kaggle/input/speech1/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveshow(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:28.604829Z","iopub.execute_input":"2022-08-15T05:14:28.605153Z","iopub.status.idle":"2022-08-15T05:14:29.572582Z","shell.execute_reply.started":"2022-08-15T05:14:28.605123Z","shell.execute_reply":"2022-08-15T05:14:29.571433Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"#  Source - RAVDESS; Gender - Male; Emotion - Happy \npath = \"/kaggle/input/speech1/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveshow(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:29.574418Z","iopub.execute_input":"2022-08-15T05:14:29.575595Z","iopub.status.idle":"2022-08-15T05:14:30.514094Z","shell.execute_reply.started":"2022-08-15T05:14:29.575546Z","shell.execute_reply":"2022-08-15T05:14:30.512688Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"/kaggle/input/speech1/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n# female1 = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale1 = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"/kaggle/input/speech1/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale1 = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale1 = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Source - RAVDESS; Gender - Female; Emotion - happy \npath = \"/kaggle/input/speech1/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - happy \npath = \"/kaggle/input/speech1/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.plot(male1, label='male1')\nplt.plot(female1, label='female1')\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:30.516098Z","iopub.execute_input":"2022-08-15T05:14:30.519471Z","iopub.status.idle":"2022-08-15T05:14:31.761533Z","shell.execute_reply.started":"2022-08-15T05:14:30.519427Z","shell.execute_reply":"2022-08-15T05:14:31.760143Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"# Source - Tess; Gender - Female; Emotion - fear \npath = \"../input/speech/TESS Toronto emotional speech set data/OAF_Fear/OAF_back_fear.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale2 = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale2 = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Source - Tess; Gender - Male; Emotion - fear \npath = \"../input/speech/TESS Toronto emotional speech set data/OAF_Fear/OAF_cab_fear.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale2 = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale2 = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female2, label='female')\nplt.plot(male2, label='male')\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:31.767178Z","iopub.execute_input":"2022-08-15T05:14:31.767808Z","iopub.status.idle":"2022-08-15T05:14:32.473596Z","shell.execute_reply.started":"2022-08-15T05:14:31.767759Z","shell.execute_reply":"2022-08-15T05:14:32.472256Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"MODEL CREATIION AND TUNING","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.475878Z","iopub.execute_input":"2022-08-15T05:14:32.476590Z","iopub.status.idle":"2022-08-15T05:14:32.485139Z","shell.execute_reply.started":"2022-08-15T05:14:32.476540Z","shell.execute_reply":"2022-08-15T05:14:32.484145Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"ref = pd.read_csv(\"/kaggle/input/data-pathcsv/Data_path.csv\")\nref.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.486535Z","iopub.execute_input":"2022-08-15T05:14:32.487144Z","iopub.status.idle":"2022-08-15T05:14:32.529729Z","shell.execute_reply.started":"2022-08-15T05:14:32.487109Z","shell.execute_reply":"2022-08-15T05:14:32.528685Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"ref.drop(ref.index[ref['source'] == 'SAVEE'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.530959Z","iopub.execute_input":"2022-08-15T05:14:32.531484Z","iopub.status.idle":"2022-08-15T05:14:32.541051Z","shell.execute_reply.started":"2022-08-15T05:14:32.531452Z","shell.execute_reply":"2022-08-15T05:14:32.539890Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"ref['source'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.542884Z","iopub.execute_input":"2022-08-15T05:14:32.543630Z","iopub.status.idle":"2022-08-15T05:14:32.554110Z","shell.execute_reply.started":"2022-08-15T05:14:32.543567Z","shell.execute_reply":"2022-08-15T05:14:32.552650Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"ref.drop(ref.index[ref['source'] == 'CREMA'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.555812Z","iopub.execute_input":"2022-08-15T05:14:32.556156Z","iopub.status.idle":"2022-08-15T05:14:32.565858Z","shell.execute_reply.started":"2022-08-15T05:14:32.556125Z","shell.execute_reply":"2022-08-15T05:14:32.564521Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"ref.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.568082Z","iopub.execute_input":"2022-08-15T05:14:32.569260Z","iopub.status.idle":"2022-08-15T05:14:32.576505Z","shell.execute_reply.started":"2022-08-15T05:14:32.569219Z","shell.execute_reply":"2022-08-15T05:14:32.575493Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"df.source.unique()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.578415Z","iopub.execute_input":"2022-08-15T05:14:32.580983Z","iopub.status.idle":"2022-08-15T05:14:32.590107Z","shell.execute_reply.started":"2022-08-15T05:14:32.580927Z","shell.execute_reply":"2022-08-15T05:14:32.588831Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(df.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df1.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:14:32.592050Z","iopub.execute_input":"2022-08-15T05:14:32.593163Z","iopub.status.idle":"2022-08-15T05:30:39.459119Z","shell.execute_reply.started":"2022-08-15T05:14:32.593113Z","shell.execute_reply":"2022-08-15T05:30:39.457580Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"markdown","source":"DF! IS THE NEW FEATURE MATRIX OF THE MFCCS OF THE AUDIO SAMPLE DONT CONFUSE DF WAL THE PATH AND FEATURE MATRIX.","metadata":{}},{"cell_type":"code","source":"df.reset_index(inplace=True, drop=True)\ndf2 = pd.concat([df,pd.DataFrame(df1['feature'].values.tolist())],axis=1)\ndf2[:5]\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:35:11.211875Z","iopub.execute_input":"2022-08-15T05:35:11.212363Z","iopub.status.idle":"2022-08-15T05:35:11.502293Z","shell.execute_reply.started":"2022-08-15T05:35:11.212328Z","shell.execute_reply":"2022-08-15T05:35:11.501175Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"df = df2","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:36:59.916711Z","iopub.execute_input":"2022-08-15T05:36:59.917178Z","iopub.status.idle":"2022-08-15T05:36:59.922891Z","shell.execute_reply.started":"2022-08-15T05:36:59.917141Z","shell.execute_reply":"2022-08-15T05:36:59.921697Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"df=df.fillna(0)\nprint(df.shape)\ndf[:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-15T05:40:31.943309Z","iopub.execute_input":"2022-08-15T05:40:31.943776Z","iopub.status.idle":"2022-08-15T05:40:31.978403Z","shell.execute_reply.started":"2022-08-15T05:40:31.943741Z","shell.execute_reply":"2022-08-15T05:40:31.977183Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","metadata":{"execution":{"iopub.status.busy":"2022-08-15T06:01:06.684018Z","iopub.execute_input":"2022-08-15T06:01:06.684509Z","iopub.status.idle":"2022-08-15T06:01:06.736597Z","shell.execute_reply.started":"2022-08-15T06:01:06.684472Z","shell.execute_reply":"2022-08-15T06:01:06.735314Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"# tried min max scaling but didnt work wel the gaussian distribution was not fairly even\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\nX_train[150:160]","metadata":{"execution":{"iopub.status.busy":"2022-08-15T06:01:10.790785Z","iopub.execute_input":"2022-08-15T06:01:10.791789Z","iopub.status.idle":"2022-08-15T06:01:10.843650Z","shell.execute_reply.started":"2022-08-15T06:01:10.791742Z","shell.execute_reply":"2022-08-15T06:01:10.842307Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n# y_train = y_train.reshape(-1, 1)\n# y_test = y_test.reshape(-1, 1)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\nimport pickle\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T06:01:11.392308Z","iopub.execute_input":"2022-08-15T06:01:11.392743Z","iopub.status.idle":"2022-08-15T06:01:11.421489Z","shell.execute_reply.started":"2022-08-15T06:01:11.392708Z","shell.execute_reply":"2022-08-15T06:01:11.420096Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape\n# 3 conv1 expects 3 dimensions so lets give it :)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T06:02:04.372947Z","iopub.execute_input":"2022-08-15T06:02:04.373822Z","iopub.status.idle":"2022-08-15T06:02:04.381256Z","shell.execute_reply.started":"2022-08-15T06:02:04.373778Z","shell.execute_reply":"2022-08-15T06:02:04.380121Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))# overfitting regularization\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n# opt = keras.optimizers.Adam(lr=0.0001)\nopt = tf.keras.optimizers.Adam(lr=0.0001,decay=1e-6)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T06:21:10.662515Z","iopub.execute_input":"2022-08-15T06:21:10.662953Z","iopub.status.idle":"2022-08-15T06:21:10.884954Z","shell.execute_reply.started":"2022-08-15T06:21:10.662911Z","shell.execute_reply":"2022-08-15T06:21:10.883680Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=16, epochs=25, validation_data=(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:14:16.004394Z","iopub.execute_input":"2022-08-15T07:14:16.004874Z","iopub.status.idle":"2022-08-15T07:27:18.556272Z","shell.execute_reply.started":"2022-08-15T07:14:16.004829Z","shell.execute_reply":"2022-08-15T07:27:18.555006Z"},"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"code","source":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:33:09.083208Z","iopub.execute_input":"2022-08-15T07:33:09.083705Z","iopub.status.idle":"2022-08-15T07:33:09.287155Z","shell.execute_reply.started":"2022-08-15T07:33:09.083663Z","shell.execute_reply":"2022-08-15T07:33:09.285929Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"code","source":"model_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:34:08.988361Z","iopub.execute_input":"2022-08-15T07:34:08.989747Z","iopub.status.idle":"2022-08-15T07:34:09.099278Z","shell.execute_reply.started":"2022-08-15T07:34:08.989701Z","shell.execute_reply":"2022-08-15T07:34:09.098307Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = tf.keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:35:59.530266Z","iopub.execute_input":"2022-08-15T07:35:59.530719Z","iopub.status.idle":"2022-08-15T07:36:03.574961Z","shell.execute_reply.started":"2022-08-15T07:35:59.530681Z","shell.execute_reply":"2022-08-15T07:36:03.573411Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"# Keras optimiser\nopt = tf.keras.optimizers.Adam(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:36:35.290694Z","iopub.execute_input":"2022-08-15T07:36:35.291755Z","iopub.status.idle":"2022-08-15T07:36:38.305032Z","shell.execute_reply.started":"2022-08-15T07:36:35.291709Z","shell.execute_reply":"2022-08-15T07:36:38.303863Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"# Keras optimiser\nopt = tf.keras.optimizers.SGD(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:36:51.545820Z","iopub.execute_input":"2022-08-15T07:36:51.546259Z","iopub.status.idle":"2022-08-15T07:36:54.627707Z","shell.execute_reply.started":"2022-08-15T07:36:51.546221Z","shell.execute_reply":"2022-08-15T07:36:54.626793Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"markdown","source":"<!-- Every optimizer is getting me the same  -->","metadata":{}},{"cell_type":"code","source":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:52:09.545998Z","iopub.execute_input":"2022-08-15T07:52:09.546478Z","iopub.status.idle":"2022-08-15T07:52:12.030464Z","shell.execute_reply.started":"2022-08-15T07:52:09.546437Z","shell.execute_reply":"2022-08-15T07:52:12.029092Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"code","source":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:52:15.084867Z","iopub.execute_input":"2022-08-15T07:52:15.085719Z","iopub.status.idle":"2022-08-15T07:52:15.104473Z","shell.execute_reply.started":"2022-08-15T07:52:15.085666Z","shell.execute_reply":"2022-08-15T07:52:15.103475Z"},"trusted":true},"execution_count":249,"outputs":[]},{"cell_type":"code","source":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=True)\nfinaldf.groupby('predictedvalues').count()\ntype(finaldf.actualvalues)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:55:15.455569Z","iopub.execute_input":"2022-08-15T07:55:15.456516Z","iopub.status.idle":"2022-08-15T07:55:15.473582Z","shell.execute_reply.started":"2022-08-15T07:55:15.456474Z","shell.execute_reply":"2022-08-15T07:55:15.472556Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"code","source":"confusion_matrix = pd.crosstab(finaldf.actualvalues, finaldf.predictedvalues, rownames=['Actual'], colnames=['Predicted'])\nprint (confusion_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:55:57.917030Z","iopub.execute_input":"2022-08-15T07:55:57.917473Z","iopub.status.idle":"2022-08-15T07:55:57.945215Z","shell.execute_reply.started":"2022-08-15T07:55:57.917435Z","shell.execute_reply":"2022-08-15T07:55:57.943975Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:56:57.393866Z","iopub.execute_input":"2022-08-15T07:56:57.395042Z","iopub.status.idle":"2022-08-15T07:56:57.576419Z","shell.execute_reply.started":"2022-08-15T07:56:57.394991Z","shell.execute_reply":"2022-08-15T07:56:57.575171Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"sn.heatmap(confusion_matrix, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:57:56.801493Z","iopub.execute_input":"2022-08-15T07:57:56.802860Z","iopub.status.idle":"2022-08-15T07:57:57.834580Z","shell.execute_reply.started":"2022-08-15T07:57:56.802802Z","shell.execute_reply":"2022-08-15T07:57:57.833437Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"classes = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:58:22.490120Z","iopub.execute_input":"2022-08-15T07:58:22.490594Z","iopub.status.idle":"2022-08-15T07:58:22.530283Z","shell.execute_reply.started":"2022-08-15T07:58:22.490547Z","shell.execute_reply":"2022-08-15T07:58:22.528974Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))","metadata":{"execution":{"iopub.status.busy":"2022-08-15T07:59:45.658075Z","iopub.execute_input":"2022-08-15T07:59:45.658534Z","iopub.status.idle":"2022-08-15T07:59:45.669050Z","shell.execute_reply.started":"2022-08-15T07:59:45.658494Z","shell.execute_reply":"2022-08-15T07:59:45.667701Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"## will try out on TRansformers and LSTM there is notch for the transformenrs","metadata":{"execution":{"iopub.status.busy":"2022-08-15T08:00:26.508247Z","iopub.execute_input":"2022-08-15T08:00:26.509043Z","iopub.status.idle":"2022-08-15T08:00:26.513946Z","shell.execute_reply.started":"2022-08-15T08:00:26.509001Z","shell.execute_reply":"2022-08-15T08:00:26.512466Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}